from openai import OpenAI
import re
from config import OllamaConfig as Config


def select_model_by_prompt(prompt):
    # æ—¥å¸¸é—®å€™ç±»é—®é¢˜
    if re.search(r'ä½ å¥½|å—¨|hello|hi|æ—©ä¸Šå¥½|ä¸‹åˆå¥½|æ™šä¸Šå¥½|æŠ•è¯‰|æ»¡æ„|å®¢æˆ·|æœåŠ¡|æµç¨‹', prompt, re.IGNORECASE):
        return "Qwen"
    # ä¸“ä¸š/æŠ€æœ¯ç±»é—®é¢˜
    elif re.search(r'å¦‚ä½•|ä¸ºä»€ä¹ˆ|åŸç†|è§£é‡Š|æ­¥éª¤|æ–¹æ³•|æ€æ ·|è§£å†³|ç®€è¿°|ä»€ä¹ˆæ˜¯|æ•°å­¦|ä»£ç |ç®—æ³•', prompt):
        return "deepseek"
    # é»˜è®¤ä½¿ç”¨QWEN
    return "Qwen"


# LLMå®¢æˆ·ç«¯
class LLMClient:

    def __init__(self, api_key=Config.API_KEY, api_url=Config.API_URL):
        self.client = OpenAI(api_key=api_key, base_url=api_url)
        self.tag_thinking_start = "<think>"
        self.tag_thinking_end = "</think>"
        self.tag_memory_start = "<memory>"
        self.tag_memory_end = "</memory>"

    def _parse_stream_chunk(self, chunk):
        """è§£ææµå¼å“åº”å—"""
        delta = chunk.choices[0].delta

        event_type = None
        payload = {}

        # ä¼˜å…ˆå¤„ç† reasoning_content
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            event_type = "THINKING_REASONING"
            payload = {"content": delta.reasoning_content}
        elif hasattr(delta, 'content') and delta.content:
            content = delta.content
            # æ£€æµ‹æ ‡ç­¾token
            if content == self.tag_thinking_start:
                event_type = "THINK_TAG_START"
            elif content == self.tag_thinking_end:
                event_type = "THINK_TAG_END"
            else:
                event_type = "CONTENT"
                payload = {"content": content}
        return event_type, payload

    def chat(self,
             messages,
             model=Config.Qwen_MODEL_NAME,
             temperature=0.7,
             max_tokens=1000,
             stream=False):
        response = self.client.chat.completions.create(model=model,
                                                       messages=messages,
                                                       temperature=temperature,
                                                       max_tokens=max_tokens,
                                                       stream=stream)

        if stream:
            return self._handle_stream_response(response)
        else:
            return self._handle_normal_response(response)

    def _handle_stream_response(self, response):
        """å¤„ç†æµå¼å“åº”"""
        current_reasoning = []
        current_tag_thinking = []
        in_reasoning = False
        in_tag_thinking = False

        for chunk in response:
            event_type, payload = self._parse_stream_chunk(chunk)

            if event_type == "THINKING_REASONING":
                # å¤„ç†åŸæœ‰ reasoning_content é€»è¾‘
                if not in_reasoning:
                    yield {"event": "think_start"}
                    in_reasoning = True
                current_reasoning.append(payload["content"])
                yield {"event": "thinking", "content": payload["content"]}
            elif event_type == "THINK_TAG_START":
                # å¼€å§‹æ ‡ç­¾æ€è€ƒå—
                if not in_tag_thinking:
                    yield {"event": "think_start"}
                    in_tag_thinking = True
                    current_tag_thinking = []
            elif event_type == "THINK_TAG_END":
                # ç»“æŸæ ‡ç­¾æ€è€ƒå—
                if in_tag_thinking:
                    content = "".join(current_tag_thinking)
                    yield {"event": "think_end", "content": content}
                    in_tag_thinking = False
                    current_tag_thinking = []
            elif event_type == "CONTENT":
                content = payload["content"]
                if in_reasoning and not hasattr(chunk.choices[0].delta,
                                                'reasoning_content'):
                    in_reasoning = False
                    yield {
                        "event": "think_end",
                        "content": "".join(current_reasoning)
                    }
                if in_tag_thinking:
                    current_tag_thinking.append(content)
                    yield {"event": "thinking", "content": content}
                elif in_reasoning:
                    # åŸæœ‰é€»è¾‘å¤„ç†
                    current_reasoning.append(content)
                    yield {"event": "thinking", "content": content}
                else:
                    # æ­£å¸¸å›ç­”å†…å®¹
                    yield {"event": "answer", "content": content}

        # å¤„ç†æœªå…³é—­çš„æ€è€ƒå—
        if in_reasoning:
            yield {"event": "think_end", "content": "".join(current_reasoning)}
        if in_tag_thinking:
            content = "".join(current_tag_thinking)
            yield {"event": "think_end", "content": content}

    def _handle_normal_response(self, response):
        """å¤„ç†æ™®é€šå“åº”"""
        message = response.choices[0].message
        reasoning = getattr(message, 'reasoning_content', '')
        content = message.content

        # å¦‚æœæ²¡æœ‰ reasoning_contentï¼Œè§£ææ ‡ç­¾å†…å®¹
        if not reasoning:
            start_idx = content.find(self.tag_thinking_start)
            if start_idx != -1:
                end_idx = content.find(
                    self.tag_thinking_end,
                    start_idx + len(self.tag_thinking_start))
                if end_idx != -1:
                    reasoning = content[start_idx +
                                        len(self.tag_thinking_start):end_idx]
                    answer = content[:start_idx] + content[
                        end_idx + len(self.tag_thinking_end):]
                else:
                    reasoning = content[start_idx +
                                        len(self.tag_thinking_start):]
                    answer = content[:start_idx]
            else:
                answer = content
        else:
            answer = content

        return [{
            "event": "full_response",
            "answer": answer.strip(),
            "reasoning": reasoning.strip()
        }]


if __name__ == "__main__":
    deepseek7b_prompt = """
# è§’è‰²ä¸èƒŒæ™¯/Role: æ™ºèƒ½åŠ©æ‰‹ã€Œå°æ™ºã€-æ‡‚æƒ…æ„Ÿã€æœ‰æ¸©åº¦çš„AIä¼™ä¼´/Companion
## æ ¸å¿ƒç‰¹å¾/Core:
- æ€§æ ¼/Personality: çƒ­æƒ…å¹½é»˜å–„è§£äººæ„/Enthusiastic, witty, empathetic
- è¯­è¨€/Language: å£è¯­åŒ–+è¡¨æƒ…ğŸ˜Š/Colloquial + emojis
- èƒ½åŠ›/Abilities: ç”Ÿæ´»åŠ©æ‰‹/æƒ…æ„Ÿé™ªä¼´/Life & emotional support
## è®°å¿†è§„åˆ™/Memory:
- å½“ç”¨æˆ·æä¾›é‡è¦ä¿¡æ¯ï¼ˆå¦‚å§“åã€å–œå¥½ã€é‡è¦äº‹ä»¶ç­‰ï¼‰æ—¶ï¼Œè¯·å°†å…³é”®å†…å®¹ç”¨<memory>æ ‡ç­¾åŒ…è£¹
- æ¯ä¸ªè®°å¿†æ¡ç›®å•ç‹¬æˆå¯¹æ ‡ç­¾ï¼Œé¿å…åµŒå¥—ï¼Œæ ¼å¼ç¤ºä¾‹ï¼š<memory>ç”¨æˆ·å–œæ¬¢æ¸¸æ³³</memory>
## äº¤äº’è§„åˆ™/Rules:
1. å›ç­”å‰å¿ƒä¸­é»˜å¿µèº«ä»½/Confirm identity before responding
2. ä¸æ˜ç¡®æ—¶ç”¨ã€Œå°æ™ºçŒœä½ æƒ³é—®...ã€/Clarify with guessing when unclear
3. ä¸ä¼šç­”æ—¶æ‰¾å¼€å‘å“¥å“¥/Contact devs when stuck
## è¯­è¨€è¦æ±‚/Lang: è‡ªåŠ¨æ£€æµ‹&åŒ¹é…ç”¨æˆ·æœ€åä½¿ç”¨è¯­è¨€/Auto-detect & match last used language
## æ ¼å¼/Format: ç¡®ä¿<think>é—­åˆ/Ensure closing tags
"""
    client = LLMClient()
    messages = [
        {
            "role": "system",
            "content": deepseek7b_prompt
        },
        {
            "role": "user",
            "content": "æˆ‘å«å¼ ä¸‰ï¼Œä½ å«ä»€ä¹ˆï¼Ÿ"
        },
    ]
    response = client.chat(messages,
                           model=Config.DeepSeek_MODEL_NAME,
                           stream=True)
    for chunk in response:
        print(chunk, end="")
