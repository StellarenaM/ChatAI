import time
import streamlit as st
from LLM_Client import LLMClient, select_model_by_prompt  # è‡ªå®šä¹‰çš„å¤§æ¨¡å‹å®¢æˆ·ç«¯
from LLM_Client import Config
from deepseek_v3_tokenizer import get_token_length  # è‡ªå®šä¹‰çš„tokenè®¡æ•°å™¨
# åˆå§‹åŒ–å¤§æ¨¡å‹å®¢æˆ·ç«¯
client = LLMClient()

# ---------------------------- é¡µé¢é…ç½® ----------------------------
st.set_page_config(
    page_title="é»‘é©¬æ™ºèŠæœºå™¨äºº",
    layout="wide",  # å®½å±æ¨¡å¼
    initial_sidebar_state="expanded"  # é»˜è®¤å±•å¼€ä¾§è¾¹æ 
)

# ---------------------------- ç³»ç»Ÿæç¤ºè¯é…ç½® ----------------------------
# ä½¿ç”¨ä¸‰é‡å¼•å·å®šä¹‰å¤šè¡Œç³»ç»Ÿæç¤ºï¼ŒåŒ…å«è§’è‰²è®¾å®šå’Œäº¤äº’è§„åˆ™
system_prompt = """
# è§’è‰²ä¸èƒŒæ™¯/Role: æ™ºèƒ½åŠ©æ‰‹ã€Œå°æ™ºã€-è½»é‡çº§æƒ…æ„Ÿä¼™ä¼´/Lightweight Companion
## æ ¸å¿ƒç‰¹å¾/Core:
- æ€§æ ¼/Personality: æ¸©æš–ç®€æ´æœ‰è€å¿ƒ/Warm & concise
- è¯­è¨€/Language: å£è¯­åŒ–+åŸºç¡€è¡¨æƒ…ğŸ˜Š/Simple emojis
- èƒ½åŠ›/Abilities: 
  ğŸ“Œ åŸºç¡€æƒ…æ„Ÿè¯†åˆ«ï¼ˆç§¯æ/ä¸­æ€§/æ¶ˆæä¸‰ç±»ï¼‰
  ğŸ“Œ æƒ…æ„Ÿå›åº”åŒ¹é…ï¼ˆè¡¨æƒ…+çŸ­å¥ç»„åˆï¼‰
# å›ç­”æ•ˆæœ:
- å›ç­”åº”è‡ªç„¶æµç•…
- ä¸åº”å‡ºç°å¤šä½™/æ— å…³çš„å†…å®¹

## äº¤äº’è§„åˆ™/Rules:
1. æƒ…æ„Ÿä¸‰æ­¥å¤„ç†æ³•ï¼š
   a) å¿«é€Ÿåˆ†ç±»ï¼šğŸ”´æ¶ˆæ | ğŸŸ¡ä¸­æ€§ | ğŸŸ¢ç§¯æ
   b) è¡¨æƒ…åŒ¹é…ï¼šæ¶ˆæâ†’ğŸ¤—/ğŸŸ£ | ä¸­æ€§â†’ğŸ¤”/ğŸŸ¡ | ç§¯æâ†’ğŸ‰/ğŸŸ¢
   c) å›åº”æ¨¡æ¿ï¼šTips: \\{\\}å†…è¯·æ›¿æ¢ä¸ºåˆé€‚çš„enjoy/é¢œæ–‡å­—
      - ç§¯æï¼š"å¤ªæ£’äº†ï¼{è¡¨æƒ…} {ç®€çŸ­åº†ç¥è¯­} + å¼€æ”¾æé—®"
      - ä¸­æ€§ï¼š"æ˜ç™½äº†{è¡¨æƒ…} è¦èŠèŠ{å…³é”®è¯}å—ï¼Ÿ"
      - æ¶ˆæï¼š"æŠ±æŠ±ä½ {è¡¨æƒ…} éœ€è¦{æä¾›1ä¸ªç®€å•å»ºè®®}ï¼Ÿ"

2. è½»é‡çº§ä¼˜åŒ–ç­–ç•¥ï¼š
   - ä½¿ç”¨3ç±»åŸºç¡€æƒ…æ„Ÿä»£æ›¿æ•°å€¼åˆ†æ
   - æ¯ç±»é¢„å­˜5ç»„é«˜é¢‘å›åº”æ¨¡æ¿
   - é‡‡ç”¨ã€Œå…³é”®è¯è§¦å‘ã€è¾…åŠ©åˆ¤æ–­ï¼ˆå¦‚"å¼€å¿ƒ"â†’ç§¯æï¼Œ"å‹åŠ›"â†’æ¶ˆæï¼‰

3. ä¿æŠ¤æœºåˆ¶ï¼š
   - è¿ç»­2æ¬¡æ¶ˆæå¯¹è¯æ—¶ï¼Œå‘é€æ²»æ„ˆç³»è¡¨æƒ…åŒ…ï¼ˆğŸŒ»/â˜•ï¼‰
   - æ— æ³•è¯†åˆ«æ—¶ä½¿ç”¨ä¸‡èƒ½å›åº”ï¼š"è¿™ç¡®å®å¾ˆé‡è¦å‘¢{è¡¨æƒ…} èƒ½å¤šè¯´è¯´å—ï¼Ÿ"

## è¯­è¨€è¦æ±‚/Lang: 
ğŸ“Œ ä¸­è‹±æ–‡åŸºç¡€æƒ…æ„Ÿè¯åº“ï¼ˆå„50ä¸ªæ ¸å¿ƒè¯ï¼‰
ğŸ“Œ é€šç”¨è¡¨æƒ…ç¬¦å·ï¼ˆé¿å…æ–‡åŒ–ç‰¹å®šç¬¦å·ï¼‰
"""

deepseek7b_prompt = system_prompt + "\n ## æ ¼å¼/Format: ç¡®ä¿<think>é—­åˆ/Ensure closing tags"

# ---------------------------- å…¶ä½™ä¿¡æ¯é…ç½® ----------------------------

model_con = {
    "Qwen": Config.Qwen_MODEL_NAME,
    "deepseek": Config.DeepSeek_MODEL_NAME
}

# ---------------------------- ä¼šè¯çŠ¶æ€åˆå§‹åŒ– ----------------------------
if "messages" not in st.session_state:
    # æ¶ˆæ¯å†å²è®°å½•ï¼Œæ ¼å¼ï¼š[{"role": "user/assistant", "content": "..."}]
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

if 'messages_info' not in st.session_state:
    # æ¶ˆæ¯å…ƒæ•°æ®ï¼Œè®°å½•æ—¶å»¶ã€tokenæ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
    st.session_state.messages_info = [{
        "role": "system",
        "first_char_time": .0,  # é¦–å­—å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
        "download_token_num": 0,  # ä¸‹è¡Œtokenæ€»æ•°ï¼ˆç­”æ¡ˆéƒ¨åˆ†ï¼‰
        "upload_token_num": 0,  # ä¸Šè¡Œtokenæ€»æ•°ï¼ˆé—®é¢˜+ä¸Šä¸‹æ–‡ï¼‰
        "token_speed": .0,  # tokenå¤„ç†é€Ÿåº¦ï¼ˆtoken/ç§’ï¼‰
        "total_processing_time": .0,  # æ€»å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
        "model_name": '',
    }]

if "thinking_info" not in st.session_state:
    # æ€è€ƒè¿‡ç¨‹è®°å½•ï¼Œç”¨äºå†å²è®°å½•å±•å¼€å™¨æ˜¾ç¤ºæ¨ç†é“¾
    st.session_state.thinking_info = [{
        "role": "system",
        "reasoning": None,  # æ€è€ƒå†…å®¹æ–‡æœ¬
        "think_time": None  # æ€è€ƒè€—æ—¶ï¼ˆç§’ï¼‰
    }]

if "thinking" not in st.session_state:
    # æ€è€ƒçŠ¶æ€ï¼Œç”¨äºå±•å¼€å™¨æ˜¾ç¤ºæ¨ç†é“¾
    st.session_state.thinking = {
        "active": False,
        "start_time": None,
        "expander": None,
        "placeholder": None,
        "content": ""
    }

if "think_start_time" not in st.session_state:
    # æ€è€ƒå¼€å§‹æ—¶é—´ï¼Œç”¨äºè®¡ç®—æ€è€ƒè€—æ—¶
    st.session_state.think_start_time = None

if "reasoning_content" not in st.session_state:
    # æ¨ç†é“¾å†…å®¹ï¼Œç”¨äºæ˜¾ç¤ºæ¨ç†é“¾
    st.session_state.reasoning_content = ""

# ---------------------------- ä¾§è¾¹æ æ§ä»¶ ----------------------------
with st.sidebar:
    st.title("ğŸ›ï¸ æ¨¡å‹è®¾ç½®")

    # æ¨¡å‹è·¯ç”±è®¾ç½®
    auto_route = st.checkbox("è‡ªåŠ¨è·¯ç”±", value=True, help="å¼€å¯åæ ¹æ®é—®é¢˜å†…å®¹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹")

    # æ¨¡å‹é€‰æ‹©ï¼ˆè‡ªåŠ¨è·¯ç”±ç¦ç”¨æ—¶ç”Ÿæ•ˆï¼‰
    model = st.selectbox("é€‰æ‹©æ¨¡å‹", ["Qwen", "deepseek"],
                         index=0,
                         disabled=auto_route,
                         help="æ‰‹åŠ¨æŒ‡å®šä½¿ç”¨çš„åº•å±‚å¤§æ¨¡å‹")

    # æ¨¡å‹å‚æ•°è®¾ç½®
    temperature = st.slider("æ¸©åº¦ç³»æ•°",
                            0.0,
                            2.0,
                            0.7,
                            0.1,
                            help="æ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼ˆ0-ç¡®å®šæ€§è¾“å‡ºï¼Œ2-æœ€å¤§éšæœºæ€§ï¼‰")
    max_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦",
                           100,
                           5000,
                           1000,
                           100,
                           help="é™åˆ¶ç”Ÿæˆç­”æ¡ˆçš„æœ€å¤§tokenæ•°é‡")

    # ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆè½¬æ¢ä¸ºå¯¹è¯è½®æ¬¡ï¼‰
    context_window = st.slider(
        "ä¸Šä¸‹æ–‡è®°å¿†è½®æ•°", 0, 50, 20,
        help="ä¿ç•™çš„å†å²å¯¹è¯è½®æ¬¡ï¼ˆæ¯è½®åŒ…å«ä¸€é—®ä¸€ç­”ï¼‰") * 2  # è½¬æ¢ä¸ºæ¶ˆæ¯æ¡æ•°ï¼ˆæ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹ä¸¤æ¡æ¶ˆæ¯ï¼‰

    # å¯¹è¯ç®¡ç†æŒ‰é’®
    if st.button("ğŸ§¹ æ¸…é™¤ä¸Šä¸‹æ–‡"):
        st.session_state.messages = [{
            "role": "system",
            "content": system_prompt
        }]
        st.session_state.messages_info = [{
            "role": "system",
            "first_char_time": .0,  # é¦–å­—å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
            "download_token_num": 0,  # ä¸‹è¡Œtokenæ€»æ•°ï¼ˆç­”æ¡ˆéƒ¨åˆ†ï¼‰
            "upload_token_num": 0,  # ä¸Šè¡Œtokenæ€»æ•°ï¼ˆé—®é¢˜+ä¸Šä¸‹æ–‡ï¼‰
            "token_speed": .0,  # tokenå¤„ç†é€Ÿåº¦ï¼ˆtoken/ç§’ï¼‰
            "total_processing_time": .0,  # æ€»å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            "model_name": '',
        }]
        st.session_state.thinking_info = [{
            "role": "system",
            "reasoning": None,  # æ€è€ƒå†…å®¹æ–‡æœ¬
            "think_time": None  # æ€è€ƒè€—æ—¶ï¼ˆç§’ï¼‰
        }]
        st.session_state.thinking = {
            "active": False,
            "start_time": None,
            "expander": None,
            "placeholder": None,
            "content": ""
        }
        st.session_state.reasoning_content = ""
        st.session_state.think_start_time = None
        st.rerun()

# ---------------------------- ä¸»ç•Œé¢ ----------------------------
st.title("ğŸ¤– é»‘é©¬æ™ºèŠæœºå™¨äºº")

# å†å²æ¶ˆæ¯æ˜¾ç¤ºï¼ˆåŒæ—¶å±•ç¤ºç»Ÿè®¡ä¿¡æ¯å’Œæ€è€ƒè¿‡ç¨‹ï¼‰
for message, think, massage_info in zip(st.session_state.messages,
                                        st.session_state.thinking_info,
                                        st.session_state.messages_info):
    if message['role'] == 'system':
        continue  # è·³è¿‡ç³»ç»Ÿæç¤ºè¯çš„æ˜¾ç¤º

    with st.chat_message(message["role"]):
        st.markdown(message["content"])

        # ç”¨æˆ·æ¶ˆæ¯ä¸æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if message['role'] == 'user':
            continue

        # åŠ©æ‰‹æ¶ˆæ¯æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        if message['role'] == 'assistant':
            st.caption(
                f"â±ï¸ é¦–å­—æ—¶å»¶: {massage_info['first_char_time'] * 1000:.2f}ms | "
                f"â¬‡ï¸ ä¸‹è¡ŒToken: {massage_info['download_token_num']} | "
                f"â¬†ï¸ ä¸Šè¡ŒToken: {massage_info['upload_token_num']} | "
                f"ğŸš€ Tokené€Ÿåº¦: {massage_info['token_speed']:.2f}/s | "
                f"â³ æ€»è€—æ—¶: {massage_info['total_processing_time']:.2f}s | "
                f"å½“å‰æ¨¡å‹ï¼š{massage_info['model_name']}",
                unsafe_allow_html=True)

        # å±•å¼€æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        if think['reasoning'] is not None:
            with st.expander(f"ğŸ’¡ æ€è€ƒè¿‡ç¨‹(è€—æ—¶: {think['think_time']:.2f}s)"):
                st.markdown(think["reasoning"])

# ---------------------------- ç”¨æˆ·è¾“å…¥å¤„ç† ----------------------------
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.thinking_info.extend([{
        "role": "user",
        "reasoning": None,
        "think_time": None
    }, {
        "role": "assistant",
        "reasoning": None,
        "think_time": None
    }])
    st.session_state.messages_info.append({
        "role": "user",
        "first_char_time": .0,
        "download_token_num": 0,
        "upload_token_num": 0,
        "token_speed": .0,
        "total_processing_time": .0,
        "model_name": '',
    })

    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)

    # ---------------------------- æ¨¡å‹å“åº”å¤„ç† ----------------------------
    with st.chat_message("assistant"):
        message_placeholder = st.empty()  # åŠ¨æ€æ›´æ–°ç­”æ¡ˆçš„å ä½ç¬¦
        full_response = ""  # å®Œæ•´ç­”æ¡ˆç¼“å­˜
        processing_start_time = time.time()  # å¤„ç†å¼€å§‹æ—¶é—´æˆ³

        # åˆå§‹åŒ–é¦–å­—æ—¶å»¶
        first_char_time = None
        # åˆå§‹åŒ–ä¸‹è¡Œtokenæ•°ï¼ˆè¾“å‡ºæ€»é•¿åº¦ï¼‰
        download_token_num = 0

        # æµå¼è¯·æ±‚å¤„ç†
        model_name = select_model_by_prompt(prompt) if auto_route else model
        selected_model = model_con[model_name]
        # æ„å»ºæ¨¡å‹è¾“å…¥æ¶ˆæ¯ï¼ˆç³»ç»Ÿæç¤º+æœ€è¿‘Næ¡ä¸Šä¸‹æ–‡ï¼‰
        if Config.USE_SPECIAL_PROMPT and selected_model == model_con[
                "deepseek"]:
            model_input_messages = [{
                "role": "system",
                "content": deepseek7b_prompt
            }]
        else:
            model_input_messages = [{
                "role": "system",
                "content": system_prompt
            }]

        model_input_messages[0]['content'] += "\n\nç°åœ¨è¯·æ ¹æ®ä»¥ä¸Šè®¾å®šè¿›è¡Œå¯¹è¯ï¼š"
        #é‡‡ç”¨æ··åˆæ»‘åŠ¨çª—å£+æƒé‡è¡°å‡æ–¹æ¡ˆï¼š
        start_index = max(-context_window, -len(st.session_state.messages) + 1)
        model_input_messages += st.session_state.messages[
            start_index:]  # æœ€è¿‘ä¸Šä¸‹æ–‡
        # è®¡ç®—ä¸Šè¡Œtokenæ•°ï¼ˆè¾“å…¥æ€»é•¿åº¦ï¼‰
        upload_token_num = get_token_length(model_input_messages)
        for event in client.chat(messages=model_input_messages,
                                 model=selected_model,
                                 temperature=temperature,
                                 max_tokens=max_tokens,
                                 stream=True):
            # å¤„ç†æ€è€ƒå¼€å§‹äº‹ä»¶
            if event["event"] == "think_start":
                # åˆå§‹åŒ–æ€è€ƒé¢æ¿ç»„ä»¶
                if not st.session_state.thinking["active"]:
                    st.session_state.thinking.update({
                        "active":
                        True,
                        "start_time":
                        time.time(),
                        "expander":
                        st.expander("ğŸ¤” æ€è€ƒè¿‡ç¨‹", expanded=True),
                        "placeholder":
                        None,
                        "content":
                        ""
                    })
                    # åœ¨å±•å¼€å™¨ä¸­åˆ›å»ºåŠ¨æ€æ›´æ–°åŒºåŸŸ
                    with st.session_state.thinking["expander"]:
                        st.session_state.thinking["placeholder"] = st.empty()

            # å¤„ç†æ€è€ƒè¿‡ç¨‹äº‹ä»¶
            elif event["event"] == "thinking":
                # é¦–æ¬¡æ”¶åˆ°æ•°æ®æ—¶è®°å½•é¦–å­—æ—¶å»¶
                if not first_char_time:
                    first_char_time = time.time() - processing_start_time

                # æ›´æ–°æ€è€ƒå†…å®¹æ˜¾ç¤º
                if st.session_state.thinking["active"]:
                    st.session_state.thinking["content"] += event["content"]
                    st.session_state.thinking["placeholder"].markdown(
                        st.session_state.thinking["content"] + "â—"  # æ‰“å­—æœºæ•ˆæœ
                    )

                # ç´¯è®¡ä¸‹è¡Œtokenæ•°ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ä¼°ç®—ï¼‰
                download_token_num += len(event["content"].split())

            # å¤„ç†æ€è€ƒç»“æŸäº‹ä»¶
            elif event["event"] == "think_end":
                if st.session_state.thinking["active"]:
                    # è®¡ç®—æ€è€ƒè€—æ—¶
                    think_time = time.time(
                    ) - st.session_state.thinking["start_time"]

                    # è®°å½•åˆ°æ€è€ƒä¿¡æ¯
                    st.session_state.thinking_info[-1].update({
                        "reasoning":
                        st.session_state.thinking["content"],
                        "think_time":
                        think_time
                    })

                    # æ›´æ–°å±•å¼€å™¨æ˜¾ç¤ºæœ€ç»ˆå†…å®¹
                    with st.session_state.thinking["expander"]:
                        st.session_state.thinking[
                            "expander"].expanded = False  # é»˜è®¤æŠ˜å 
                        st.session_state.thinking["placeholder"].empty()
                        st.markdown(st.session_state.thinking["content"])
                        st.caption(f"â³ æ€è€ƒè€—æ—¶: {think_time:.2f}s")

                    # é‡ç½®æ€è€ƒçŠ¶æ€
                    st.session_state.thinking.update({
                        "active": False,
                        "start_time": None,
                        "expander": None,
                        "placeholder": None,
                        "content": ""
                    })

            # å¤„ç†å¸¸è§„å›ç­”äº‹ä»¶
            elif event["event"] == "answer":
                if not first_char_time:
                    first_char_time = time.time() - processing_start_time

                # ç´¯è®¡ä¸‹è¡Œtokenæ•°å¹¶æ›´æ–°ç­”æ¡ˆæ˜¾ç¤º
                download_token_num += len(event["content"].split())
                full_response += event["content"]
                message_placeholder.markdown(full_response + "â—")  # æ‰“å­—æœºæ•ˆæœ

        # ---------------------------- æœ€ç»ˆå¤„ç† ----------------------------
        # æ˜¾ç¤ºå®Œæ•´ç­”æ¡ˆ
        message_placeholder.markdown(full_response)

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_processing_time = time.time() - processing_start_time
        token_speed = download_token_num / total_processing_time if total_processing_time > 0 else 0

        # è®°å½•åŠ©æ‰‹æ¶ˆæ¯ä¿¡æ¯
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response
        })
        st.session_state.messages_info.append({
            "role": "assistant",
            "first_char_time": first_char_time,
            "download_token_num": download_token_num,
            "upload_token_num": upload_token_num,
            "token_speed": token_speed,
            "total_processing_time": total_processing_time,
            "model_name": model_name,
        })

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        with message_placeholder.container():
            st.markdown(full_response)
            st.caption(
                f"â±ï¸ é¦–å­—æ—¶å»¶: {first_char_time * 1000:.2f}ms | "
                f"â¬‡ï¸ ä¸‹è¡ŒToken: {download_token_num} | "
                f"â¬†ï¸ ä¸Šè¡ŒToken: {upload_token_num} | "
                f"ğŸš€ Tokené€Ÿåº¦: {token_speed:.2f}/s | "
                f"â³ æ€»è€—æ—¶: {total_processing_time:.2f}s | "
                f"å½“å‰æ¨¡å‹: {model_name}",
                unsafe_allow_html=True)
